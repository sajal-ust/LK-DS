{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_excel(\"stockist_data_with_date3 3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure Date column is properly converted to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')  # 'coerce' will convert invalid dates to NaT\n",
    "\n",
    "# Drop rows with invalid dates if any\n",
    "df = df.dropna(subset=['Date'])\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values(by='Date')\n",
    "\n",
    "# Extract relevant product categories (using your original column names)\n",
    "expected_product_columns = [\n",
    "    'AIS(Air Insulated Switchgear)', 'RMU(Ring Main Unit)', \n",
    "    'PSS(Compact Sub-Stations)', 'VCU(Vacuum Contactor Units)', \n",
    "    'E-House', 'VCB(Vacuum Circuit Breaker)', \n",
    "    'ACB(Air Circuit Breaker)', 'MCCB(Moduled Case Circuit Breaker)', \n",
    "    'SDF(Switch Disconnectors)', 'BBT(Busbar Trunking)', \n",
    "    'Modular Switches', 'Starter', 'Controller', \n",
    "    'Solar Solutions', 'Pump Starter and Controller'\n",
    "]\n",
    "\n",
    "# Find which of these columns actually exist in the dataframe\n",
    "product_columns = [col for col in expected_product_columns if col in df.columns]\n",
    "\n",
    "# Create Product_id based on which product has value 1\n",
    "df['Product_id'] = df[product_columns].idxmax(axis=1)\n",
    "\n",
    "# Selecting necessary columns\n",
    "df = df[['Partner_id', 'Product_id', 'Date', 'MRP', 'Sales_Quantity_Last_Period', \n",
    "         'Discount_Applied', 'Geography', 'Competitor_Price']].dropna()\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={\n",
    "    'MRP': 'Price', \n",
    "    'Sales_Quantity_Last_Period': 'Demand', \n",
    "    'Discount_Applied': 'Discount'\n",
    "}, inplace=True)\n",
    "\n",
    "# Filter valid data\n",
    "df = df[(df['Price'] > 0) & (df['Demand'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare and clean the input dataframe with proper date handling\"\"\"\n",
    "    # Convert and validate dates\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df = df.dropna(subset=['Date']).sort_values('Date')\n",
    "    \n",
    "    # Create proper monthly index (ensuring correct period conversion)\n",
    "    df['YearMonth'] = pd.to_datetime(df['Date'].dt.to_period('M').astype(str))\n",
    "\n",
    "    # Set as datetime index\n",
    "    df.set_index('YearMonth', inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "    # Aggregate to monthly level\n",
    "    monthly_df = df.groupby(['Product_id', 'Geography', 'YearMonth']).agg({\n",
    "        'Demand': 'sum',\n",
    "        'Price': 'mean',\n",
    "        'Competitor_Price': 'mean',\n",
    "        'Discount': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Log transformations with proper handling\n",
    "    for col in ['Demand', 'Price', 'Competitor_Price']:\n",
    "        monthly_df[f'Log_{col}'] = np.log1p(monthly_df[col])  # Using log1p to handle zeros\n",
    "    \n",
    "    return monthly_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "monthly_df = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def train_test_split_monthly(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Time-based train-test split that maintains temporal order\n",
    "    \"\"\"\n",
    "    # Convert to datetime if not already\n",
    "    df['YearMonth'] = pd.to_datetime(df['YearMonth'])\n",
    "    \n",
    "    # Get all unique months and sort them\n",
    "    unique_months = pd.Series(df['YearMonth'].unique()).sort_values().values\n",
    "    \n",
    "    # Calculate split point\n",
    "    split_idx = int(len(unique_months) * (1 - test_size))\n",
    "    train_months = unique_months[:split_idx]\n",
    "    test_months = unique_months[split_idx:]\n",
    "    \n",
    "    # Split the data\n",
    "    train_df = df[df['YearMonth'].isin(train_months)]\n",
    "    test_df = df[df['YearMonth'].isin(test_months)]\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "\n",
    "# from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# def train_sarimax_model(train_group):\n",
    "#     \"\"\"Train SARIMAX model on training data only\"\"\"\n",
    "#     try:\n",
    "#         # Prevent modifying original data\n",
    "#         train_group = train_group.copy().sort_values('YearMonth')\n",
    "\n",
    "#         print(\"Min values:\\n\", train_group[['Log_Demand', 'Log_Price', 'Log_Competitor_Price']].min())\n",
    "#         print(\"Max values:\\n\", train_group[['Log_Demand', 'Log_Price', 'Log_Competitor_Price']].max())\n",
    "#         print(\"Check for NaN:\\n\", train_group.isna().sum())\n",
    "        \n",
    "\n",
    "#         # Ensure proper index format\n",
    "#         train_group['YearMonth'] = pd.to_datetime(train_group['YearMonth'])\n",
    "#         train_group.set_index('YearMonth', inplace=True)\n",
    "#         # train_group = train_group.resample('MS').asfreq()\n",
    "        \n",
    "#         train_group.index = pd.date_range(start=train_group.index.min(), \n",
    "#                                   periods=len(train_group), \n",
    "#                                   freq='MS')\n",
    "\n",
    "#         # Extract target variable\n",
    "#         y_train = train_group['Log_Demand'].dropna()\n",
    "#         print(\"Unique time periods:\", len(y_train))\n",
    "\n",
    "#         # Extract exogenous variables (ensure no missing values)\n",
    "#         exog_cols = ['Log_Price', 'Log_Competitor_Price']\n",
    "#         exog_train = train_group[exog_cols].dropna()\n",
    "\n",
    "#         # Skip if insufficient training data\n",
    "#         if len(y_train) < 24:\n",
    "#             print(\"Skipping training: Insufficient data.\")\n",
    "#             return None\n",
    "\n",
    "#         # Train SARIMAX model\n",
    "#         model = SARIMAX(\n",
    "#             y_train,\n",
    "#             exog=exog_train if not exog_train.empty else None,  # Handle case where exog might be empty\n",
    "#             order=(1, 1, 1),\n",
    "#             seasonal_order=(0, 1, 1, 6),\n",
    "#             enforce_stationarity=False,\n",
    "#             enforce_invertibility=False \n",
    "#         )\n",
    "#         # fitted_model = model.fit(disp=False)\n",
    "#         fitted_model = model.fit(method='powell', disp=False)\n",
    "\n",
    "\n",
    "#         return fitted_model\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Training failed: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "def train_sarimax_model(train_group):\n",
    "    \"\"\"Train SARIMAX model on training data only\"\"\"\n",
    "    try:\n",
    "        # Prevent modifying original data\n",
    "        train_group = train_group.copy().sort_values('YearMonth')\n",
    "\n",
    "        # # Debugging checks\n",
    "        print(\"Min values:\\n\", train_group[['Log_Demand', 'Log_Price', 'Log_Competitor_Price']].min())\n",
    "        print(\"Max values:\\n\", train_group[['Log_Demand', 'Log_Price', 'Log_Competitor_Price']].max())\n",
    "        print(\"Check for NaN:\\n\", train_group.isna().sum())\n",
    "\n",
    "        # Ensure proper index format\n",
    "        train_group['YearMonth'] = pd.to_datetime(train_group['YearMonth'])\n",
    "        train_group.set_index('YearMonth', inplace=True)\n",
    "\n",
    "        # Ensure monthly frequency without modifying actual timestamps\n",
    "        train_group = train_group.asfreq('MS')\n",
    "\n",
    "        # Extract target variable\n",
    "        y_train = train_group['Log_Demand']\n",
    "        # y_train = train_group['Log_Demand'].dropna()\n",
    "        # print(\"Unique time periods:\", len(y_train))\n",
    "\n",
    "        # Extract exogenous variables, filling missing values\n",
    "        exog_cols = ['Log_Price', 'Log_Competitor_Price']\n",
    "        exog_train = train_group[exog_cols].ffill()  # Forward fill exog vars\n",
    "\n",
    "\n",
    "        # Skip if insufficient training data\n",
    "        if len(y_train) < 24:\n",
    "            print(\"Skipping training: Insufficient data.\")\n",
    "            return None\n",
    "\n",
    "        # Train SARIMAX model\n",
    "        model = SARIMAX(\n",
    "            y_train,\n",
    "            exog=exog_train if not exog_train.empty else None,  \n",
    "            order=(1, 1, 1),\n",
    "            seasonal_order=(0, 1, 1, 12),  # 12-month seasonality (was 6)\n",
    "            enforce_stationarity=True,  # Force stability\n",
    "            enforce_invertibility=True   # Prevent large errors\n",
    "        )\n",
    "\n",
    "        start_params = [\n",
    "        0.5,    # AR(1)\n",
    "        0.3,    # MA(1)\n",
    "        0.2,    # Seasonal MA(1)\n",
    "        0.1,    # Intercept\n",
    "        0.05,   # Coef for exog 1\n",
    "        -0.05,  # Coef for exog 2\n",
    "        1.0     # sigma¬≤ (variance of errors)\n",
    "        ]\n",
    "        # Fit model with more stable optimization\n",
    "        fitted_model = model.fit()\n",
    "        # fitted_model = model.fit(start_params = start_params, method='lbfgs', maxiter=200, disp=False)\n",
    "\n",
    "        return fitted_model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def calculate_elasticity(fitted_model):\n",
    "    \"\"\"Calculates price elasticity from fitted model\"\"\"\n",
    "    try:\n",
    "        # Get elasticity (coefficient of Log_Price)\n",
    "        if hasattr(fitted_model, 'params') and 'Log_Price' in fitted_model.params:\n",
    "            return fitted_model.params['Log_Price']\n",
    "        return np.nan\n",
    "    except Exception as e:\n",
    "        print(f\"Elasticity calculation failed: {str(e)}\")\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "def evaluate_model(model, test_group):\n",
    "    \"\"\"Evaluate model performance on test data\"\"\"\n",
    "    try:\n",
    "        # Clean test data\n",
    "        # test_group = clean_log_values(test_group.copy())\n",
    "        test_group = test_group.sort_values('YearMonth')\n",
    "        \n",
    "        # Prepare test data\n",
    "        y_test = test_group.set_index('YearMonth')['Log_Demand']\n",
    "        exog_test = test_group.set_index('YearMonth')[['Log_Price', 'Log_Competitor_Price']]\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecast = model.get_forecast(\n",
    "            steps=len(y_test),\n",
    "            exog=exog_test\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics (converting back from log scale)\n",
    "        pred = np.exp(forecast.predicted_mean).values  # Convert to numpy array\n",
    "        actual = np.exp(y_test).values  # Convert to numpy array\n",
    "        \n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        # with warnings.catch_warnings():\n",
    "        #     warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "        mape = np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape,\n",
    "            'actual': actual,\n",
    "            'predicted': pred\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "def generate_forecasts(model, train_group, steps=12):\n",
    "    \"\"\"Generate future forecasts with proper unit handling\"\"\"\n",
    "    try:\n",
    "        # Get last available data point\n",
    "        last_data = train_group.sort_values('YearMonth').iloc[-1]\n",
    "        \n",
    "        # Create future exogenous variables\n",
    "        future_exog = pd.DataFrame(\n",
    "            [last_data[['Log_Price', 'Log_Competitor_Price']].values] * steps,\n",
    "            columns=['Log_Price', 'Log_Competitor_Price'],\n",
    "            index=pd.date_range(\n",
    "                start=train_group['YearMonth'].max() + pd.offsets.MonthBegin(1),\n",
    "                periods=steps,\n",
    "                freq='MS'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecast = model.get_forecast(\n",
    "            steps=steps,\n",
    "            exog=future_exog\n",
    "        ).predicted_mean\n",
    "        \n",
    "        # Return in original scale (only convert if model used log)\n",
    "        if 'Log_Demand' in model.model.endog_names:\n",
    "            return np.exp(forecast)\n",
    "        return forecast\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Forecast generation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_forecast_pipeline(monthly_df):\n",
    "    \"\"\"Complete forecasting pipeline\"\"\"\n",
    "    # 1. Split data (time-based)\n",
    "    train_df, test_df = train_test_split_monthly(monthly_df, test_size=0.2)\n",
    "    \n",
    "    detailed_results = []\n",
    "    performance_metrics = []\n",
    "    \n",
    "    for (product, region), group in train_df.groupby(['Product_id', 'Geography']):\n",
    "        # Get corresponding test data\n",
    "        test_group = test_df[\n",
    "            (test_df['Product_id'] == product) & \n",
    "            (test_df['Geography'] == region)\n",
    "        ]\n",
    "        \n",
    "        # Skip if no test data\n",
    "        if len(test_group) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Train model\n",
    "        # model = train_sarimax_model(group)\n",
    "        # model_results = model  # already trained SARIMAXResultsWrapper object\n",
    "        # exog_vars = ['Log_Price', 'Log_Competitor_Price']  # List of exogenous variable names\n",
    "        # exog_train = group.set_index('YearMonth')[exog_vars].ffill()  # DataFrame of training exogenous features\n",
    "        # test_group = test_df[(test_df['Product_id'] == product) & (test_df['Geography'] == region)]\n",
    "        # exog_test = test_group.set_index('YearMonth')[exog_vars]  # DataFrame of testing exogenous features\n",
    "        # geography = region  # for clarity\n",
    "\n",
    "        model = train_sarimax_model(group)\n",
    "        model_results = model  # already trained SARIMAXResultsWrapper object\n",
    "\n",
    "        exog_vars = ['Log_Price', 'Log_Competitor_Price']\n",
    "        exog_train = group.set_index('YearMonth')[exog_vars].ffill()\n",
    "        test_group = test_df[(test_df['Product_id'] == product) & (test_df['Geography'] == region)]\n",
    "        exog_test = test_group.set_index('YearMonth')[exog_vars]\n",
    "        geography = region\n",
    "\n",
    "        # üîΩ Save all components in a bundle\n",
    "        import os, pickle\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        model_bundle = {\n",
    "                'model_results': model_results,\n",
    "                'exog_train': exog_train,\n",
    "                'exog_test': exog_test,\n",
    "                'exog_vars': exog_vars,\n",
    "                'product': product,\n",
    "                'geography': geography }\n",
    "        pickle.dump(model_bundle, open(f\"models/model_{product}_{region}.pkl\", \"wb\"))\n",
    "        if model is None:\n",
    "            continue\n",
    "\n",
    "        elasticity = calculate_elasticity(model)\n",
    "        \n",
    "        # Evaluate model\n",
    "        eval_results = evaluate_model(model, test_group)\n",
    "        if eval_results is None:\n",
    "            continue\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts = generate_forecasts(model, group)\n",
    "        \n",
    "        # Store results\n",
    "        detailed_results.append({\n",
    "            'product': product,\n",
    "            'region': region,\n",
    "            'model': model,\n",
    "            'actual': eval_results['actual'],\n",
    "            'predicted': eval_results['predicted'],\n",
    "            'forecast': forecasts, \n",
    "            'elasticity': elasticity\n",
    "        })\n",
    "        \n",
    "        performance_metrics.append({\n",
    "            'product': product,\n",
    "            'region': region,\n",
    "            'mae': eval_results['mae'],\n",
    "            'rmse': eval_results['rmse'],\n",
    "            'mape': eval_results['mape']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(performance_metrics), detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your monthly data (ensure it has Log_Demand, Log_Price, Log_Competitor_Price columns)\n",
    "monthly_df = prepare_data(df)\n",
    "\n",
    "# Run the full pipeline\n",
    "metrics_df, detailed_results = run_forecast_pipeline(monthly_df)\n",
    "\n",
    "# Analyze results\n",
    "print(metrics_df)  # Performance metrics for all product-region combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(metrics_df, detailed_results, base_filename=\"forecast_results\"):\n",
    "    \"\"\"\n",
    "    Save forecast results to CSV files\n",
    "    - Ensures proper date formatting\n",
    "    - Preserves original units (no double conversion)\n",
    "    - Creates clean, analysis-ready outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Save performance metrics\n",
    "    metrics_df.to_csv(f\"{base_filename}_metrics.csv\", index=False)\n",
    "    \n",
    "    # 2. Save forecasts (12-month predictions)\n",
    "    forecast_data = []\n",
    "    elasticity_data = []\n",
    "    for result in detailed_results:\n",
    "        if result['forecast'] is None:\n",
    "            continue\n",
    "            \n",
    "        # Create future dates starting next month\n",
    "        last_date = result['actual'].index[-1] if isinstance(result['actual'], pd.Series) else pd.Timestamp.now()\n",
    "        dates = pd.date_range(\n",
    "            start=last_date + pd.offsets.MonthBegin(1),\n",
    "            periods=len(result['forecast']),\n",
    "            freq='MS'\n",
    "        )\n",
    "        \n",
    "        forecast_data.append(pd.DataFrame({\n",
    "            'product': result['product'],\n",
    "            'region': result['region'],\n",
    "            'date': dates.strftime('%Y-%m-%d'),\n",
    "            'forecast': result['forecast']  # Already in original units\n",
    "        }))\n",
    "        # Elasticity DataFrame (simple product-region mapping)\n",
    "        if 'elasticity' in result:\n",
    "            elasticity_data.append({\n",
    "                'product': result['product'],\n",
    "                'region': result['region'], \n",
    "                'price_elasticity': result['elasticity']\n",
    "            })\n",
    "    \n",
    "    if forecast_data:\n",
    "        pd.concat(forecast_data).to_csv(f\"{base_filename}_forecasts.csv\", index=False)\n",
    "    \n",
    "    # 3. Save actual vs predicted comparisons\n",
    "    comparison_data = []\n",
    "    for result in detailed_results:\n",
    "        if result.get('actual') is None or result.get('predicted') is None:\n",
    "            continue\n",
    "            \n",
    "        # Get existing dates or create default range\n",
    "        if isinstance(result['actual'], pd.Series):\n",
    "            dates = result['actual'].index\n",
    "        else:\n",
    "            dates = pd.date_range(\n",
    "                end=pd.Timestamp.now(),\n",
    "                periods=len(result['actual']),\n",
    "                freq='MS'\n",
    "            )\n",
    "        \n",
    "        comparison_data.append(pd.DataFrame({\n",
    "            'product': result['product'],\n",
    "            'region': result['region'],\n",
    "            'elasticity': result['elasticity'],\n",
    "            'date': dates.strftime('%Y-%m-%d'),\n",
    "            'actual': result['actual'],  # Already in original units\n",
    "            'predicted': result['predicted']  # Already in original units\n",
    "        }))\n",
    "    \n",
    "    if comparison_data:\n",
    "        pd.concat(comparison_data).to_csv(f\"{base_filename}_comparisons.csv\", index=False)\n",
    "\n",
    "    if elasticity_data:\n",
    "        pd.DataFrame(elasticity_data).to_csv(f\"{base_filename}_elasticity.csv\", index=False)\n",
    "    \n",
    "    print(f\"Successfully saved results to:\")\n",
    "    print(f\"- Metrics: {base_filename}_metrics.csv\")\n",
    "    print(f\"- Forecasts: {base_filename}_forecasts.csv\")\n",
    "    print(f\"- Comparisons: {base_filename}_comparisons.csv\")\n",
    "    print(f\"- Elasticity: {base_filename}_elasticity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved results to:\n",
      "- Metrics: my_product_forecasts3_metrics.csv\n",
      "- Forecasts: my_product_forecasts3_forecasts.csv\n",
      "- Comparisons: my_product_forecasts3_comparisons.csv\n",
      "- Elasticity: my_product_forecasts3_elasticity.csv\n"
     ]
    }
   ],
   "source": [
    "# With custom filename:\n",
    "save_results_to_csv(metrics_df, detailed_results, \"my_product_forecasts3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_price_discount_simulation(product_region_data, elasticity_value, product_name, region_name):\n",
    "    \"\"\"\n",
    "    Create simulation table with:\n",
    "    - 5 equidistant price buckets between min/max observed prices\n",
    "    - Average demand calculated for each price bucket\n",
    "    - Demand projections at 0-50% discounts (5% intervals)\n",
    "    \n",
    "    Args:\n",
    "        product_region_data: DataFrame with historical transactions\n",
    "        elasticity_value: Pre-calculated elasticity coefficient\n",
    "        product_name: Product to analyze\n",
    "        region_name: Region to analyze\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with simulation scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter for product-region\n",
    "    data = product_region_data[\n",
    "        (product_region_data['Product_id'] == product_name) & \n",
    "        (product_region_data['Geography'] == region_name)\n",
    "    ].copy()\n",
    "    \n",
    "    if data.empty:\n",
    "        print(f\"No data for {product_name} in {region_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Create 4 price buckets\n",
    "    min_price = data['Price'].min()\n",
    "    max_price = data['Price'].max()\n",
    "    price_buckets = np.linspace(min_price, max_price, 5)\n",
    "    \n",
    "    # Calculate average demand per price bucket\n",
    "    data['price_bucket'] = pd.cut(data['Price'], bins=price_buckets, include_lowest=True)\n",
    "    bucket_stats = data.groupby('price_bucket').agg(\n",
    "        avg_demand=('Demand', 'mean'),\n",
    "        price_midpoint=('Price', lambda x: (x.min() + x.max())/2)\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Generate all discount scenarios (0-50% in 5% steps)\n",
    "    discount_rates = np.arange(0, 55, 5)\n",
    "    \n",
    "    # Build simulation scenarios\n",
    "    simulation_results = []\n",
    "    for _, bucket in bucket_stats.iterrows():\n",
    "        for discount in discount_rates:\n",
    "            new_demand = calculate_new_demand(\n",
    "                demand=bucket['avg_demand'],\n",
    "                elasticity=elasticity_value,\n",
    "                discount=discount\n",
    "            )\n",
    "            \n",
    "            simulation_results.append({\n",
    "                'product': product_name,\n",
    "                'region': region_name,\n",
    "                'price_bucket': f\"{bucket['price_bucket'].left:.2f}-{bucket['price_bucket'].right:.2f}\",\n",
    "                'price_midpoint': round(bucket['price_midpoint'], 2),\n",
    "                'discount_pct': discount,\n",
    "                'original_avg_demand': round(bucket['avg_demand'], 2),\n",
    "                'predicted_demand': round(new_demand, 2),\n",
    "                'demand_change_pct': round((new_demand - bucket['avg_demand'])/bucket['avg_demand']*100, 1),\n",
    "                'elasticity_used': round(elasticity_value, 3)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(simulation_results)\n",
    "\n",
    "def calculate_new_demand(demand, elasticity, discount):\n",
    "    \"\"\"Calculate demand response to price changes\"\"\"\n",
    "    discount = np.clip(discount, 0, 50)\n",
    "    if elasticity is None or np.isnan(elasticity):\n",
    "        elasticity = -0.2\n",
    "    return demand * np.exp(elasticity * np.log(1 - discount/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data and elasticity\n",
    "elasticity_df = pd.read_csv(\"my_product_forecasts3_elasticity.csv\")\n",
    "product_elasticity = elasticity_df[\n",
    "    (elasticity_df['product'] == \"RMU(Ring Main Unit)\") & \n",
    "    (elasticity_df['region'] == \"West\")\n",
    "]['price_elasticity'].values[0]\n",
    "\n",
    "# Generate simulation\n",
    "simulation = create_price_discount_simulation(\n",
    "    product_region_data=monthly_df,\n",
    "    elasticity_value=product_elasticity,\n",
    "    product_name=\"RMU(Ring Main Unit)\",\n",
    "    region_name=\"West\"\n",
    ")\n",
    "\n",
    "# Save results\n",
    "simulation.to_csv(\"rmu_west_simulation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_discounts(simulation_df):\n",
    "    \"\"\"\n",
    "    Calculate profit-optimizing discount at each price point\n",
    "    Returns DataFrame with optimal discount scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate revenue for each scenario\n",
    "    simulation_df['revenue'] = (\n",
    "        simulation_df['price_midpoint'] * \n",
    "        (1 - simulation_df['discount_pct']/100) * \n",
    "        simulation_df['predicted_demand']\n",
    "    )\n",
    "    \n",
    "    # Find discount that maximizes revenue for each price bucket\n",
    "    optimal_discounts = (\n",
    "        simulation_df.loc[simulation_df.groupby('price_bucket')['revenue'].idxmax()]\n",
    "        [['product', 'region', 'price_bucket', 'price_midpoint', \n",
    "          'discount_pct', 'predicted_demand', 'revenue']]\n",
    "        .sort_values('price_midpoint')\n",
    "        .rename(columns={\n",
    "            'discount_pct': 'optimal_discount_pct',\n",
    "            'predicted_demand': 'optimal_demand',\n",
    "            'revenue': 'max_revenue'\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    return optimal_discounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sign of elasticity - \n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_optimal_discounts(simulation_df, filename=\"optimal_discounts.csv\"):\n",
    "    \"\"\"Calculate and save optimal discount scenarios\"\"\"\n",
    "    optimal_df = calculate_optimal_discounts(simulation_df)\n",
    "    \n",
    "    # Format output\n",
    "    optimal_df['price_midpoint'] = optimal_df['price_midpoint'].round(2)\n",
    "    optimal_df['max_revenue'] = optimal_df['max_revenue'].round(2)\n",
    "    \n",
    "    # Save to CSV\n",
    "    optimal_df.to_csv(filename, index=False)\n",
    "    print(f\"Saved optimal discounts to {filename}\")\n",
    "    return optimal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved optimal discounts to rmu_west_optimal_discounts.csv\n",
      "                product region     price_bucket  price_midpoint  \\\n",
      "0   RMU(Ring Main Unit)   West  2537.96-2784.72         2659.17   \n",
      "11  RMU(Ring Main Unit)   West  2784.72-3031.48         2908.06   \n",
      "22  RMU(Ring Main Unit)   West  3031.48-3278.24         3180.61   \n",
      "33  RMU(Ring Main Unit)   West  3278.24-3525.00         3404.03   \n",
      "\n",
      "    optimal_discount_pct  optimal_demand  max_revenue  \n",
      "0                      0         1168.08   3106123.29  \n",
      "11                     0         1365.86   3972002.83  \n",
      "22                     0         1284.83   4086543.15  \n",
      "33                     0         1434.75   4883932.04  \n"
     ]
    }
   ],
   "source": [
    "# 2. Calculate and save optimal discounts\n",
    "optimal_discounts = save_optimal_discounts(\n",
    "    simulation,\n",
    "    filename=\"rmu_west_optimal_discounts.csv\"\n",
    ")\n",
    "\n",
    "# Display sample results\n",
    "print(optimal_discounts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from shap import sample as shap_sample\n",
    "\n",
    "def generate_shap_explanation(exog_train, exog_test, model_results, exog_vars, product, geography, sample_size=100, save_dir=\"shap_outputs\"):\n",
    "    \"\"\"\n",
    "    Generate and visualize SHAP values for a SARIMAX model.\n",
    "\n",
    "    Parameters:\n",
    "    - exog_train: DataFrame of training exogenous variables\n",
    "    - exog_test: DataFrame of testing exogenous variables\n",
    "    - model_results: Trained SARIMAX model result object\n",
    "    - exog_vars: List of exogenous variable names\n",
    "    - product: Product identifier (for title/file name)\n",
    "    - geography: Geography identifier (for title/file name)\n",
    "    - sample_size: Sample size for SHAP explanation (default: 100)\n",
    "    - save_dir: Directory to save the SHAP plot (default: 'shap_outputs')\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame of SHAP feature importances\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure sample size is feasible\n",
    "    sample_size = min(sample_size, len(exog_test))\n",
    "    if sample_size < 30:\n",
    "        print(f\"Skipping SHAP for ({product}, {geography}): Not enough test data.\")\n",
    "        return None\n",
    "\n",
    "    # Sample background data\n",
    "    background = shap_sample(exog_train, sample_size)\n",
    "\n",
    "    # Define model prediction wrapper\n",
    "    def model_predict(X):\n",
    "        X_df = pd.DataFrame(X, columns=exog_vars)\n",
    "        preds = model_results.get_prediction(start=0, end=len(X_df)-1, exog=X_df)\n",
    "        return preds.predicted_mean.values\n",
    "\n",
    "    # Initialize SHAP explainer\n",
    "    explainer = shap.KernelExplainer(model_predict, background)\n",
    "   # Compute SHAP values\n",
    "    shap_values = explainer.shap_values(exog_test[:sample_size], nsamples=sample_size)\n",
    "\n",
    "    # Calculate mean absolute SHAP values\n",
    "    importances = np.abs(shap_values).mean(axis=0)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': exog_vars,\n",
    "        'Mean_Abs_SHAP': importances\n",
    "    }).sort_values(by='Mean_Abs_SHAP', ascending=True)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"shap_bar_summary_{product}_{geography}.png\")\n",
    "   # Plot\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Mean_Abs_SHAP'])\n",
    "    plt.xlabel(\"Mean |SHAP value|\")\n",
    "    plt.title(f\"SHAP Bar Summary: {product} - {geography}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_results\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# üîº Load the model bundle\n",
    "with open(f\"models/model_{product}_{geography}.pkl\", \"rb\") as f:\n",
    "    bundle = pickle.load(f)\n",
    "\n",
    "# üîÅ Unpack the bundle\n",
    "model_results = bundle['model_results']\n",
    "exog_train = bundle['exog_train']\n",
    "exog_test = bundle['exog_test']\n",
    "exog_vars = bundle['exog_vars']\n",
    "product = bundle['product']\n",
    "geography = bundle['geography']\n",
    "\n",
    "# ‚úÖ Call SHAP explanation function\n",
    "shap_df = generate_shap_explanation(\n",
    "    exog_train=exog_train,\n",
    "    exog_test=exog_test,\n",
    "    model_results=model_results,\n",
    "    exog_vars=exog_vars,\n",
    "    product=product,\n",
    "    geography=geography,\n",
    "    sample_size=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
