# -*- coding: utf-8 -*-
"""Direct_Lambda_Handler (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17x-H-qlSw19QUQYUtaAcgeTi0w3-zD7g
"""

ACTIVE_MODULE = "recommendation"  # or "evaluation"
ACTIVE_APPROACH = "user_based"    # or "item_based"

import os
import logging
import boto3
import pandas as pd
import numpy as np
from io import BytesIO
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import jaccard_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix

# -------------------- Logging Setup --------------------
logger = logging.getLogger()
logger.setLevel(logging.INFO)
if not logger.handlers:
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

# -------------------- Config via ENV --------------------
active_approach = os.getenv("ACTIVE_APPROACH", "user_based")  # "user_based" or "item_based"
is_lambda = os.environ.get("IS_LAMBDA", "false").lower() == "true"
s3_client = boto3.client("s3")

# -------------------- I/O Helpers --------------------
def load_file_from_s3(bucket, key):
    logger.info(f"Loading file from S3: {bucket}/{key}")
    response = s3_client.get_object(Bucket=bucket, Key=key)
    return pd.read_csv(BytesIO(response['Body'].read()))

def save_file_to_s3(df, bucket, key):
    logger.info(f"Saving result to S3: {bucket}/{key}")
    buffer = BytesIO()
    df.to_csv(buffer, index=False)
    buffer.seek(0)
    s3_client.put_object(Bucket=bucket, Key=key, Body=buffer)

def load_file_locally(path):
    logger.info(f"Loading file locally: {path}")
    return pd.read_csv(path)

def save_file_locally(df, path):
    logger.info(f"Saving result locally: {path}")
    df.to_csv(path, index=False)

# -------------------- Approach 1: Item-Based (Jaccard) --------------------
def run_item_based_recommendation(df):
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
    grouped_schemes = train_df.groupby('Scheme_Type')['Product_id'].apply(set).reset_index()

    mlb = MultiLabelBinarizer()
    scheme_matrix = pd.DataFrame(
        mlb.fit_transform(grouped_schemes['Product_id']),
        index=grouped_schemes['Scheme_Type'],
        columns=mlb.classes_
    )

    similarity_matrix = pd.DataFrame(index=scheme_matrix.index, columns=scheme_matrix.index, dtype=float)
    for i in range(len(scheme_matrix)):
        for j in range(len(scheme_matrix)):
            similarity_matrix.iloc[i, j] = (
                jaccard_score(scheme_matrix.iloc[i], scheme_matrix.iloc[j]) if i != j else 1.0
            )

    recommendation = []
    for product in test_df["Product_id"].unique():
        product_schemes = test_df[test_df["Product_id"] == product]["Scheme_Type"].unique()
        for scheme in product_schemes:
            if scheme in similarity_matrix.index:
                similar_schemes = similarity_matrix.loc[scheme].drop(scheme).sort_values(ascending=False).head(3)
                recommendation.append({
                    "Product_id": product,
                    "Similarity_Scores": round(similar_schemes.mean(), 6),
                    "Scheme_1": similar_schemes.index[0] if len(similar_schemes) > 0 else None,
                    "Scheme_2": similar_schemes.index[1] if len(similar_schemes) > 1 else None,
                    "Scheme_3": similar_schemes.index[2] if len(similar_schemes) > 2 else None,
                })

    recommendation_df = pd.DataFrame(recommendation)
    recommendation_df = test_df[["Partner_id", "Product_id"]].drop_duplicates().merge(
        recommendation_df, on="Product_id", how="inner"
    )
    return recommendation_df

# -------------------- Approach 2: User-Based --------------------
def run_user_based_recommendation(df):
    df["Engagement_Score"] = np.log1p(df["Sales_Value_Last_Period"]) * (
        df["Feedback_Score"] + df["Growth_Percentage"]
    )
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df["Partner_id"])

    user_scheme_matrix = train_df.pivot_table(
        index="Partner_id", columns="Scheme_Type", values="Engagement_Score", aggfunc="sum", fill_value=0
    )

    partner_id_lookup = list(user_scheme_matrix.index)
    user_scheme_sparse = csr_matrix(user_scheme_matrix.values)

    knn_model = NearestNeighbors(metric='cosine', algorithm='brute')
    knn_model.fit(user_scheme_sparse)

    def recommend_user_based(partner_id, top_n=3):
        if partner_id not in user_scheme_matrix.index:
            return None

        idx = partner_id_lookup.index(partner_id)
        distances, indices = knn_model.kneighbors(user_scheme_sparse[idx], n_neighbors=min(top_n + 1, len(user_scheme_matrix)))
        similarities = 1 - distances.flatten()
        neighbors = indices.flatten()

        filtered = [(i, sim) for i, sim in zip(neighbors, similarities) if i != idx]
        if not filtered:
            return None

        top_idx, sim_score = filtered[0]
        similar_user = partner_id_lookup[top_idx]
        sim_score = round(sim_score, 6)

        top_schemes = (
            train_df[train_df["Partner_id"] == similar_user]["Scheme_Type"]
            .value_counts().head(3).index.tolist()
        )
        while len(top_schemes) < 3:
            top_schemes.append("No Scheme")
        product = train_df[train_df["Partner_id"] == partner_id]["Product_id"].unique()[0]

        return [partner_id, product, sim_score, *top_schemes]

    user_partners = test_df["Partner_id"].unique()
    user_recommendations = [recommend_user_based(pid) for pid in user_partners if recommend_user_based(pid)]

    user_rec_df = pd.DataFrame(
        user_recommendations,
        columns=["Partner_id", "Product_id", "Similarity_Score", "Scheme_1", "Scheme_2", "Scheme_3"]
    )
    return user_rec_df

# -------------------- Lambda Handler --------------------
def lambda_handler(event, context):
    input_bucket = "lk-scheme-recommendations"
    output_bucket = "lk-scheme-recommendations"
    input_key = "input_data/Augmented_Stockist_data.csv"

    output_key_map = {
        "item_based": "output_data/Item_Based_Scheme_Recommendations.csv",
        "user_based": "output_data/User_Based_Scheme_Recommendations.csv"
    }

    try:
        df = load_file_from_s3(input_bucket, input_key) if is_lambda else load_file_locally("Augmented_Stockist_data.csv")

        if active_approach == "item_based":
            result_df = run_item_based_recommendation(df)
        elif active_approach == "user_based":
            result_df = run_user_based_recommendation(df)
        else:
            raise ValueError(f"Unknown approach '{active_approach}' passed via environment variable.")

        output_key = output_key_map[active_approach]

        if is_lambda:
            save_file_to_s3(result_df, output_bucket, output_key)
        else:
            save_file_locally(result_df, os.path.basename(output_key))

        logger.info(f"{active_approach} scheme recommendation completed successfully.")
        return {
            "statusCode": 200,
            "body": f"{active_approach} scheme recommendation completed successfully."
        }

    except Exception as e:
        logger.error(f"Error in Lambda execution: {str(e)}")
        return {
            "statusCode": 500,
            "body": str(e)
        }

# -------------------- Local Execution --------------------
if __name__ == "__main__":
    os.environ["IS_LAMBDA"] = "false"
    os.environ["ACTIVE_APPROACH"] = "item_based"  # or "user_based"
    response = lambda_handler({}, None)
    print(response)

# -------------------- Local Execution --------------------
if __name__ == "__main__":
    os.environ["IS_LAMBDA"] = "false"
    os.environ["ACTIVE_APPROACH"] = "user_based"  # or "user_based"
    response = lambda_handler({}, None)
    print(response)



"""Evaluation

"""

import os
import pandas as pd
import boto3
from io import BytesIO
import logging

# ------------------ Logging Setup ------------------
logger = logging.getLogger()
logger.setLevel(logging.INFO)
if not logger.handlers:
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logger.addHandler(stream_handler)

# ------------------ ENV Setup ------------------
is_lambda = os.getenv("IS_LAMBDA", "false").lower() == "true"
bucket_name = os.getenv("BUCKET_NAME", "")
active_module = os.getenv("ACTIVE_MODULE", "evaluation")  # default to evaluation
test_file_key = "test_data.csv"
rec_file_key = "Scheme_Recommendations.csv"
output_file = "Scheme_Evaluation_Metrics.csv"

# ------------------ S3 Helpers ------------------
def read_csv_from_s3(key):
    s3 = boto3.client("s3")
    obj = s3.get_object(Bucket=bucket_name, Key=key)
    return pd.read_csv(BytesIO(obj["Body"].read()))

def save_csv_to_s3(df, key):
    s3 = boto3.client("s3")
    buffer = BytesIO()
    df.to_csv(buffer, index=False)
    buffer.seek(0)
    s3.put_object(Bucket=bucket_name, Key=key, Body=buffer.getvalue())

# ------------------ Evaluation Logic ------------------
def evaluate_scheme_recommendations(test_df, rec_df):
    availed_df = (
        test_df.groupby("Partner_id")["Scheme_Type"]
        .apply(list)
        .reset_index()
        .rename(columns={"Scheme_Type": "Availed_Schemes"})
    )

    rec_df["Recommended_Schemes"] = rec_df[["Scheme_1", "Scheme_2", "Scheme_3"]].values.tolist()

    df_all = pd.merge(
        availed_df,
        rec_df[["Partner_id", "Recommended_Schemes"]],
        on="Partner_id",
        how="left"
    )

    df_all["Availed_Schemes"] = df_all["Availed_Schemes"].apply(lambda x: x if isinstance(x, list) else [])
    df_all["Recommended_Schemes"] = df_all["Recommended_Schemes"].apply(lambda x: x if isinstance(x, list) else [])

    results = []
    for k in [1, 2, 3]:
        precision_list, recall_list = [], []

        for _, row in df_all.iterrows():
            actual_set = set(row["Availed_Schemes"])
            if not actual_set:
                continue

            recommended_k = row["Recommended_Schemes"][:k]
            tp = sum([1 for scheme in recommended_k if scheme in actual_set])
            precision = tp / k
            recall = tp / len(actual_set)

            precision_list.append(precision)
            recall_list.append(recall)

        avg_precision = round(sum(precision_list) / len(precision_list), 4) if precision_list else 0
        avg_recall = round(sum(recall_list) / len(recall_list), 4) if recall_list else 0
        f1 = round(2 * avg_precision * avg_recall / (avg_precision + avg_recall), 4) if (avg_precision + avg_recall) else 0

        results.append({
            "Top-K": k,
            "Avg Precision": avg_precision,
            "Avg Recall": avg_recall,
            "Avg F1 Score": f1
        })

    return pd.DataFrame(results)

# ------------------ Lambda Handler ------------------
def lambda_handler(event=None, context=None):
    try:
        logger.info("===== Starting Lambda Evaluation Handler =====")

        if active_module != "evaluation":
            logger.info("ACTIVE_MODULE is not 'evaluation' — skipping execution.")
            return {
                "statusCode": 200,
                "body": "Evaluation module not executed. ACTIVE_MODULE is not set to 'evaluation'."
            }

        # Load input
        if is_lambda:
            logger.info("Loading input from S3...")
            test_df = read_csv_from_s3(test_file_key)
            rec_df = read_csv_from_s3(rec_file_key)
        else:
            logger.info("Loading input from local...")
            test_df = pd.read_csv(test_file_key)
            rec_df = pd.read_csv(rec_file_key)

        # Evaluate
        result_df = evaluate_scheme_recommendations(test_df, rec_df)

        # Save & Print
        if is_lambda:
            save_csv_to_s3(result_df, output_file)
            logger.info(f"Saved evaluation metrics to S3: {output_file}")
        else:
            result_df.to_csv(output_file, index=False)
            logger.info(f"Saved evaluation metrics locally: {output_file}")

            # Print table
            print("\n==== Final Evaluation Table ====\n")
            print(result_df.to_string(index=False))

        return {
            "statusCode": 200,
            "body": "Evaluation completed successfully."
        }

    except Exception as e:
        logger.error(f"Error during evaluation: {str(e)}")
        return {
            "statusCode": 500,
            "body": str(e)
        }

# ------------------ Local Trigger ------------------
if __name__ == "__main__" and not is_lambda:
    lambda_handler()

# export IS_LAMBDA=false
# export ACTIVE_MODULE=evaluation
# python3 evaluation_module.py


