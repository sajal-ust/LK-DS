# -*- coding: utf-8 -*-
"""Direct_Lambda_handler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rzvSjeJUDTn_4wP6XrdPldxYaURZ5WkK
"""

import os
import logging
import boto3
import pandas as pd
import numpy as np
from io import BytesIO
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import jaccard_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix

# -------------------- Logging Setup --------------------
logger = logging.getLogger()
logger.setLevel(logging.INFO)
if not logger.handlers:
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

# -------------------- ENV Config --------------------
active_module = os.getenv("ACTIVE_MODULE", "recommendation")
active_approach = os.getenv("ACTIVE_APPROACH", "user_based")
is_lambda = os.getenv("IS_LAMBDA", "false").lower() == "true"
bucket_name = os.getenv("BUCKET_NAME", "lk-scheme-recommendations")

logger.info(f"[ENV] IS_LAMBDA={is_lambda}, ACTIVE_MODULE={active_module}, ACTIVE_APPROACH={active_approach}, BUCKET_NAME={bucket_name}")

# -------------------- I/O Helpers --------------------
s3_client = boto3.client("s3")

def load_file_from_s3(bucket, key):
    logger.info(f"Loading file from S3: {bucket}/{key}")
    response = s3_client.get_object(Bucket=bucket, Key=key)
    return pd.read_csv(BytesIO(response['Body'].read()))

def save_file_to_s3(df, bucket, key):
    logger.info(f"Saving result to S3: {bucket}/{key}")
    buffer = BytesIO()
    df.to_csv(buffer, index=False)
    buffer.seek(0)
    s3_client.put_object(Bucket=bucket, Key=key, Body=buffer)

def load_file_locally(path):
    logger.info(f"Loading file locally: {path}")
    return pd.read_csv(path)

def save_file_locally(df, path):
    logger.info(f"Saving result locally: {path}")
    df.to_csv(path, index=False)

# -------------------- Recommendation Logic --------------------
def run_item_based_recommendation(df):
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
    grouped = train_df.groupby('Scheme_Type')['Product_id'].apply(set).reset_index()
    mlb = MultiLabelBinarizer()
    scheme_matrix = pd.DataFrame(
        mlb.fit_transform(grouped['Product_id']),
        index=grouped['Scheme_Type'],
        columns=mlb.classes_
    )
    similarity_matrix = pd.DataFrame(index=scheme_matrix.index, columns=scheme_matrix.index, dtype=float)
    for i in range(len(scheme_matrix)):
        for j in range(len(scheme_matrix)):
            similarity_matrix.iloc[i, j] = jaccard_score(scheme_matrix.iloc[i], scheme_matrix.iloc[j]) if i != j else 1.0

    recommendation = []
    for product in test_df["Product_id"].unique():
        product_schemes = test_df[test_df["Product_id"] == product]["Scheme_Type"].unique()
        for scheme in product_schemes:
            if scheme in similarity_matrix.index:
                similar_schemes = similarity_matrix.loc[scheme].drop(scheme).sort_values(ascending=False).head(3)
                recommendation.append({
                    "Product_id": product,
                    "Similarity_Scores": round(similar_schemes.mean(), 6),
                    "Scheme_1": similar_schemes.index[0] if len(similar_schemes) > 0 else None,
                    "Scheme_2": similar_schemes.index[1] if len(similar_schemes) > 1 else None,
                    "Scheme_3": similar_schemes.index[2] if len(similar_schemes) > 2 else None,
                })

    recommendation_df = pd.DataFrame(recommendation)

    # ✅ Safely merge Partner_id if it exists
    if "Partner_id" in test_df.columns:
        partner_product_map = test_df[["Partner_id", "Product_id"]].drop_duplicates()
        recommendation_df = partner_product_map.merge(recommendation_df, on="Product_id", how="inner")
    else:
        logger.warning("⚠️ 'Partner_id' not found in test_df. Skipping merge.")
        recommendation_df["Partner_id"] = "UNKNOWN"

    return recommendation_df

def run_user_based_recommendation(df):
    df["Engagement_Score"] = np.log1p(df["Sales_Value_Last_Period"]) * (df["Feedback_Score"] + df["Growth_Percentage"])
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df["Partner_id"])
    matrix = train_df.pivot_table(index="Partner_id", columns="Scheme_Type", values="Engagement_Score", aggfunc="sum", fill_value=0)
    user_scheme_sparse = csr_matrix(matrix.values)
    partner_ids = list(matrix.index)
    knn = NearestNeighbors(metric='cosine', algorithm='brute')
    knn.fit(user_scheme_sparse)

    def recommend_for_user(pid, top_n=3):
        if pid not in matrix.index:
            return None
        idx = partner_ids.index(pid)
        distances, indices = knn.kneighbors(user_scheme_sparse[idx], n_neighbors=min(top_n + 1, len(matrix)))
        sims = 1 - distances.flatten()
        neighbors = indices.flatten()
        filtered = [(i, s) for i, s in zip(neighbors, sims) if i != idx]
        if not filtered:
            return None
        top_idx, sim = filtered[0]
        similar_user = partner_ids[top_idx]
        top_schemes = train_df[train_df["Partner_id"] == similar_user]["Scheme_Type"].value_counts().head(3).index.tolist()
        while len(top_schemes) < 3:
            top_schemes.append("No Scheme")
        product = train_df[train_df["Partner_id"] == pid]["Product_id"].unique()[0]
        return [pid, product, round(sim, 6), *top_schemes]

    recs = [recommend_for_user(pid) for pid in test_df["Partner_id"].unique() if recommend_for_user(pid)]
    return pd.DataFrame(recs, columns=["Partner_id", "Product_id", "Similarity_Score", "Scheme_1", "Scheme_2", "Scheme_3"])

# -------------------- Recommendation Handler --------------------
def recommendation_handler(event=None, context=None):
    input_key = "Augmented_Stockist_Dat.csv"
    output_map = {
        "item_based": "output_data/Item_Based_Scheme_Recommendations.csv",
        "user_based": "output_data/User_Based_Scheme_Recommendations.csv"
    }
    try:
        df = load_file_from_s3(bucket_name, input_key) if is_lambda else load_file_locally("Augmented_Stockist_Dat.csv")
        result_df = run_item_based_recommendation(df) if active_approach == "item_based" else run_user_based_recommendation(df)
        output_key = output_map[active_approach]
        if is_lambda:
            save_file_to_s3(result_df, bucket_name, output_key)
        else:
            save_file_locally(result_df, os.path.basename(output_key))
        logger.info(f"{active_approach} scheme recommendation completed successfully.")
        return {"statusCode": 200, "body": f"{active_approach} scheme recommendation completed successfully."}
    except Exception as e:
        logger.error(f"Error in Lambda execution: {str(e)}")
        return {"statusCode": 500, "body": str(e)}

# -------------------- Evaluation Logic --------------------
def read_csv_from_s3(key):
    obj = s3_client.get_object(Bucket=bucket_name, Key=key)
    return pd.read_csv(BytesIO(obj["Body"].read()))

def save_csv_to_s3(df, key):
    buffer = BytesIO()
    df.to_csv(buffer, index=False)
    buffer.seek(0)
    s3_client.put_object(Bucket=bucket_name, Key=key, Body=buffer.getvalue())

def evaluate_scheme_recommendations(test_df, rec_df):
    test_df = test_df.groupby("Partner_id")["Scheme_Type"].apply(list).reset_index().rename(columns={"Scheme_Type": "Availed_Schemes"})
    rec_df["Recommended_Schemes"] = rec_df[["Scheme_1", "Scheme_2", "Scheme_3"]].values.tolist()
    merged = pd.merge(test_df, rec_df[["Partner_id", "Recommended_Schemes"]], on="Partner_id", how="left")
    merged["Availed_Schemes"] = merged["Availed_Schemes"].apply(lambda x: x if isinstance(x, list) else [])
    merged["Recommended_Schemes"] = merged["Recommended_Schemes"].apply(lambda x: x if isinstance(x, list) else [])
    results = []
    for k in [1, 2, 3]:
        precisions, recalls = [], []
        for _, row in merged.iterrows():
            actual = set(row["Availed_Schemes"])
            if not actual:
                continue
            predicted = row["Recommended_Schemes"][:k]
            tp = len(set(predicted) & actual)
            precisions.append(tp / k)
            recalls.append(tp / len(actual))
        ap = round(sum(precisions) / len(precisions), 4) if precisions else 0
        ar = round(sum(recalls) / len(recalls), 4) if recalls else 0
        f1 = round(2 * ap * ar / (ap + ar), 4) if ap + ar else 0
        results.append({"Top-K": k, "Avg Precision": ap, "Avg Recall": ar, "Avg F1 Score": f1})
    return pd.DataFrame(results)

def evaluation_handler(event=None, context=None):
    try:
        logger.info("===== Starting Lambda Evaluation Handler =====")
        test_file = "test_data.csv"
        rec_file = "Scheme_Recommendations.csv"
        output_file = "Scheme_Evaluation_Metrics.csv"
        test_df = read_csv_from_s3(test_file) if is_lambda else pd.read_csv(test_file)
        rec_df = read_csv_from_s3(rec_file) if is_lambda else pd.read_csv(rec_file)
        result_df = evaluate_scheme_recommendations(test_df, rec_df)
        if is_lambda:
            save_csv_to_s3(result_df, output_file)
        else:
            result_df.to_csv(output_file, index=False)
            print("\n==== Final Evaluation Table ====\n")
            print(result_df.to_string(index=False))
        return {"statusCode": 200, "body": "Evaluation completed successfully."}
    except Exception as e:
        logger.error(f"Error during evaluation: {str(e)}")
        return {"statusCode": 500, "body": str(e)}

# -------------------- Main Trigger --------------------
if __name__ == "__main__":
    if active_module == "recommendation":
        response = recommendation_handler({}, None)
        print(response)
    elif active_module == "evaluation":
        response = evaluation_handler()
        print(response)
    else:
        logger.error(f"Unknown ACTIVE_MODULE: {active_module}")

import os
import logging
import boto3
import pandas as pd
import numpy as np
from io import BytesIO
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import jaccard_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix

# -------------------- Logging Setup --------------------
logger = logging.getLogger()
logger.setLevel(logging.INFO)
if not logger.handlers:
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

# -------------------- ENV Config --------------------
active_module = os.getenv("ACTIVE_MODULE", "recommendation")
active_approach = os.getenv("ACTIVE_APPROACH", "user_based")
is_lambda = os.getenv("IS_LAMBDA", "false").lower() == "true"
bucket_name = os.getenv("BUCKET_NAME", "lk-scheme-recommendations")

logger.info(f"[ENV] IS_LAMBDA={is_lambda}, ACTIVE_MODULE={active_module}, ACTIVE_APPROACH={active_approach}, BUCKET_NAME={bucket_name}")

# -------------------- I/O Helpers --------------------
s3_client = boto3.client("s3")

def load_file_from_s3(bucket, key):
    logger.info(f"Loading file from S3: {bucket}/{key}")
    response = s3_client.get_object(Bucket=bucket, Key=key)
    return pd.read_csv(BytesIO(response['Body'].read()))

def save_file_to_s3(df, bucket, key):
    logger.info(f"Saving result to S3: {bucket}/{key}")
    buffer = BytesIO()
    df.to_csv(buffer, index=False)
    buffer.seek(0)
    s3_client.put_object(Bucket=bucket, Key=key, Body=buffer)

def load_file_locally(path):
    logger.info(f"Loading file locally: {path}")
    return pd.read_csv(path)

def save_file_locally(df, path):
    logger.info(f"Saving result locally: {path}")
    df.to_csv(path, index=False)

# -------------------- Item-Based Recommendation --------------------
def run_item_based_recommendation(df):
    df["Engagement_Score"] = np.log1p(df["Sales_Value_Last_Period"]) * (df["Feedback_Score"] + df["Growth_Percentage"])
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
    partner_product_schemes = train_df.groupby(["Partner_id", "Product_id"])["Scheme_Type"].apply(list).reset_index()
    partner_product_schemes["Entity"] = partner_product_schemes["Partner_id"] + "_" + partner_product_schemes["Product_id"]
    mlb = MultiLabelBinarizer()
    scheme_matrix = pd.DataFrame(
        mlb.fit_transform(partner_product_schemes["Scheme_Type"]),
        index=partner_product_schemes["Entity"],
        columns=mlb.classes_
    ).T
    similarity_matrix = pd.DataFrame(index=scheme_matrix.index, columns=scheme_matrix.index, dtype=float)
    for i in range(len(scheme_matrix)):
        for j in range(len(scheme_matrix)):
            similarity_matrix.iloc[i, j] = jaccard_score(scheme_matrix.iloc[i], scheme_matrix.iloc[j]) if i != j else 1.0
    test_pairs = test_df[["Partner_id", "Product_id", "Scheme_Type"]].drop_duplicates()
    recommendations = []
    for _, row in test_pairs.iterrows():
        partner, product, current_scheme = row
        if current_scheme in similarity_matrix.index:
            similar_schemes = similarity_matrix.loc[current_scheme].drop(current_scheme).sort_values(ascending=False).head(3)
            sim_list = similar_schemes.index.tolist()
            recommendations.append({
                "Partner_id": partner,
                "Product_id": product,
                "Similarity_Score": round(similar_schemes.mean(), 6),
                "Scheme_1": sim_list[0] if len(sim_list) > 0 else "No Scheme",
                "Scheme_2": sim_list[1] if len(sim_list) > 1 else "No Scheme",
                "Scheme_3": sim_list[2] if len(sim_list) > 2 else "No Scheme"
            })
    return pd.DataFrame(recommendations)

# -------------------- User-Based Recommendation --------------------
def run_user_based_recommendation(df):
    df["Engagement_Score"] = np.log1p(df["Sales_Value_Last_Period"]) * (df["Feedback_Score"] + df["Growth_Percentage"])
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df["Partner_id"])
    matrix = train_df.pivot_table(index="Partner_id", columns="Scheme_Type", values="Engagement_Score", aggfunc="mean", fill_value=0)
    user_scheme_sparse = csr_matrix(matrix.values)
    partner_ids = list(matrix.index)
    knn = NearestNeighbors(metric='cosine', algorithm='brute')
    knn.fit(user_scheme_sparse)
    def recommend_for_user(pid, top_n=3):
        if pid not in matrix.index:
            return None
        idx = partner_ids.index(pid)
        distances, indices = knn.kneighbors(user_scheme_sparse[idx], n_neighbors=min(top_n + 1, len(matrix)))
        sims = 1 - distances.flatten()
        neighbors = indices.flatten()
        filtered = [(i, s) for i, s in zip(neighbors, sims) if i != idx]
        if not filtered:
            return None
        top_idx, sim = filtered[0]
        similar_user = partner_ids[top_idx]
        top_schemes = train_df[train_df["Partner_id"] == similar_user]["Scheme_Type"].value_counts().head(3).index.tolist()
        while len(top_schemes) < 3:
            top_schemes.append("No Scheme")
        product = train_df[train_df["Partner_id"] == pid]["Product_id"].unique()[0]
        return [pid, product, round(sim, 6), *top_schemes]
    recs = [recommend_for_user(pid) for pid in test_df["Partner_id"].unique() if recommend_for_user(pid)]
    return pd.DataFrame(recs, columns=["Partner_id", "Product_id", "Similarity_Score", "Scheme_1", "Scheme_2", "Scheme_3"])

# -------------------- Evaluation Logic --------------------
def evaluate_scheme_recommendations(test_df, rec_df):
    test_df = test_df.groupby("Partner_id")["Scheme_Type"].apply(list).reset_index().rename(columns={"Scheme_Type": "Availed_Schemes"})
    rec_df["Recommended_Schemes"] = rec_df[["Scheme_1", "Scheme_2", "Scheme_3"]].values.tolist()
    merged = pd.merge(test_df, rec_df[["Partner_id", "Recommended_Schemes"]], on="Partner_id", how="left")
    merged["Availed_Schemes"] = merged["Availed_Schemes"].apply(lambda x: x if isinstance(x, list) else [])
    merged["Recommended_Schemes"] = merged["Recommended_Schemes"].apply(lambda x: x if isinstance(x, list) else [])
    results = []
    for k in [1, 2, 3]:
        precisions, recalls = [], []
        for _, row in merged.iterrows():
            actual = set(row["Availed_Schemes"])
            if not actual:
                continue
            predicted = row["Recommended_Schemes"][:k]
            tp = len(set(predicted) & actual)
            precisions.append(tp / k)
            recalls.append(tp / len(actual))
        ap = round(sum(precisions) / len(precisions), 4) if precisions else 0
        ar = round(sum(recalls) / len(recalls), 4) if recalls else 0
        f1 = round(2 * ap * ar / (ap + ar), 4) if ap + ar else 0
        results.append({"Top-K": k, "Avg Precision": ap, "Avg Recall": ar, "Avg F1 Score": f1})
    return pd.DataFrame(results)

# -------------------- Handlers --------------------
def recommendation_handler(event=None, context=None):
    input_key = "Augmented_Stockist_Dat.csv"
    output_map = {
        "item_based": "output_data/Item_Based_Scheme_Recommendations.csv",
        "user_based": "output_data/User_Based_Scheme_Recommendations.csv"
    }
    try:
        df = load_file_from_s3(bucket_name, input_key) if is_lambda else load_file_locally(input_key)
        result_df = run_item_based_recommendation(df) if active_approach == "item_based" else run_user_based_recommendation(df)
        output_key = output_map[active_approach]
        if is_lambda:
            save_file_to_s3(result_df, bucket_name, output_key)
        else:
            save_file_locally(result_df, os.path.basename(output_key))
        logger.info(f"{active_approach} scheme recommendation completed successfully.")
        return {"statusCode": 200, "body": f"{active_approach} scheme recommendation completed successfully."}
    except Exception as e:
        logger.error(f"Error in Lambda execution: {str(e)}")
        return {"statusCode": 500, "body": str(e)}

def evaluation_handler(event=None, context=None):
    try:
        logger.info("===== Starting Lambda Evaluation Handler =====")
        test_file = "test_data.csv"
        rec_file = {
            "item_based": "output_data/Item_Based_Scheme_Recommendations.csv",
            "user_based": "output_data/User_Based_Scheme_Recommendations.csv"
        }[active_approach]
        output_file = "Scheme_Evaluation_Metrics.csv"
        test_df = load_file_from_s3(bucket_name, test_file) if is_lambda else pd.read_csv(test_file)
        rec_df = load_file_from_s3(bucket_name, rec_file) if is_lambda else pd.read_csv(rec_file)
        result_df = evaluate_scheme_recommendations(test_df, rec_df)
        if is_lambda:
            save_file_to_s3(result_df, bucket_name, output_file)
        else:
            result_df.to_csv(output_file, index=False)
            print("\n==== Final Evaluation Table ====\n")
            print(result_df.to_string(index=False))
        return {"statusCode": 200, "body": "Evaluation completed successfully."}
    except Exception as e:
        logger.error(f"Error during evaluation: {str(e)}")
        return {"statusCode": 500, "body": str(e)}

# -------------------- Main Trigger --------------------
if __name__ == "__main__":
    if active_module == "recommendation":
        response = recommendation_handler({}, None)
        print(response)
    elif active_module == "evaluation":
        response = evaluation_handler()
        print(response)
    else:
        logger.error(f"Unknown ACTIVE_MODULE: {active_module}")

