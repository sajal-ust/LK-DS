# -*- coding: utf-8 -*-
"""Item_Based.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PVWeWnhPXQG2aMsgH1lC4WoL9fYJQ3dq
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import jaccard_score
from sklearn.model_selection import train_test_split

#Load Dataset
df = pd.read_csv("Augmented_Stockist_data.csv")

# Split the dataset into training (80%) and testing (20%) sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

df

# # One-hot encoding for Geography and Stockist_Type
# train_df = pd.get_dummies(train_df, columns=["Geography", "Stockist_Type"], dtype=int)

# # Identify geography and stockist type columns
# geo_columns = [col for col in train_df.columns if col.startswith("Geography")]
# stockist_columns = [col for col in train_df.columns if col.startswith("Stockist_Type")]

# if not geo_columns or not stockist_columns:
#     raise ValueError("No Geography or Stockist_Type features found after encoding! Check encoding step.")

# # Ensure Sales_Value_Last_Period does not contain zeros to avoid log(0)
# train_df["Sales_Value_Last_Period"] = train_df["Sales_Value_Last_Period"].replace(0, 1)

# Extract Scheme-Product Relationships from Training Data
# Group products under each scheme from the training set
grouped_schemes = train_df.groupby('Scheme_Type')['Product_id'].apply(set).reset_index()

grouped_schemes

# Convert Scheme-Product Associations into a Binary Matrix
mlb = MultiLabelBinarizer()
scheme_matrix = pd.DataFrame(
    mlb.fit_transform(grouped_schemes['Product_id']),
    index=grouped_schemes['Scheme_Type'],
    columns=mlb.classes_
)

mlb

# Compute Jaccard Similarity Matrix
similarity_matrix = pd.DataFrame(index=scheme_matrix.index, columns=scheme_matrix.index, dtype=float)

for i in range(len(scheme_matrix)):
    for j in range(len(scheme_matrix)):
        if i != j:
            similarity_matrix.iloc[i, j] = jaccard_score(scheme_matrix.iloc[i], scheme_matrix.iloc[j])
        else:
            similarity_matrix.iloc[i, j] = 1.0  # Self-similarity

similarity_matrix

print(scheme_matrix.head())

# Scheme Recommendation Function
def recommend_similar_schemes(scheme_name, top_n=3):
    """
    Recommends the top N similar schemes based on Jaccard similarity scores.
    Parameters:
    scheme_name (str): The name of the scheme for which recommendations are needed.
    top_n (int): The number of similar schemes to return.
    Returns:
    list: A list of the top N recommended schemes.
    """
    if scheme_name not in similarity_matrix.index:
        return f"Scheme '{scheme_name}' not found in data."

    similarities = similarity_matrix.loc[scheme_name].drop(scheme_name)
    recommended_schemes = similarities.sort_values(ascending=False).head(top_n).index.tolist()
    return recommended_schemes

# Compute Scheme Recommendations on Test Set

recommendation = []

for product in test_df["Product_id"].unique():
    # Find all schemes related to this product in the test set
    product_schemes = test_df[test_df["Product_id"] == product]["Scheme_Type"].unique()

    for scheme in product_schemes:
        if scheme in similarity_matrix.index:
            # Get the top 3 similar schemes for the given scheme
            similar_schemes = similarity_matrix.loc[scheme].drop(scheme).sort_values(ascending=False).head(3)

            recommendation.append({
                "Product_id": product,
                "Similarity_Scores": round(similar_schemes.mean(), 6),  # Average similarity score
                "Scheme_1": similar_schemes.index[0] if len(similar_schemes) > 0 else None,
                "Scheme_2": similar_schemes.index[1] if len(similar_schemes) > 1 else None,
                "Scheme_3": similar_schemes.index[2] if len(similar_schemes) > 2 else None,
            })

# Convert Recommendations to DataFrame
recommendation_df = pd.DataFrame(recommendation)

# Merge with original test dataset to include Partner_id for better context
recommendation_df = test_df[["Partner_id", "Product_id"]].drop_duplicates().merge(
    recommendation_df, on="Product_id", how="inner"
)

recommendation_df

# Save the Recommendations
recommendation_df.to_csv("Scheme_Recommendations.csv", index=False)



