{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40f3e93f-8808-42a8-943f-8c3e9a54ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Augmented_Stockist_Data.csv\")\n",
    "\n",
    "# Split into train and test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df.to_csv(\"Train_Data.csv\", index=False)\n",
    "test_df.to_csv(\"Test_Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b639bcdc-f4f0-4e0f-9a5b-7e06cb6c3911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group schemes applied per (Partner, Product)\n",
    "partner_product_schemes = train_df.groupby([\"Partner_id\", \"Product_id\"])[\"Scheme_Type\"].apply(list).reset_index()\n",
    "partner_product_schemes[\"Entity\"] = partner_product_schemes[\"Partner_id\"] + \"_\" + partner_product_schemes[\"Product_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51473d75-b0e7-471c-b482-50271fd7b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encode scheme presence per entity\n",
    "mlb = MultiLabelBinarizer()\n",
    "scheme_matrix = pd.DataFrame(\n",
    "    mlb.fit_transform(partner_product_schemes[\"Scheme_Type\"]),\n",
    "    index=partner_product_schemes[\"Entity\"],\n",
    "    columns=mlb.classes_\n",
    ").T\n",
    "\n",
    "# Jaccard similarity between schemes\n",
    "similarity_matrix = pd.DataFrame(index=scheme_matrix.index, columns=scheme_matrix.index, dtype=float)\n",
    "for i in range(len(scheme_matrix)):\n",
    "    for j in range(len(scheme_matrix)):\n",
    "        if i != j:\n",
    "            similarity_matrix.iloc[i, j] = jaccard_score(scheme_matrix.iloc[i], scheme_matrix.iloc[j])\n",
    "        else:\n",
    "            similarity_matrix.iloc[i, j] = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d1b4039-9a4d-4421-aad8-e2ad3a361373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top-3 similar scheme recommendations per test (Partner, Product, Scheme)\n",
    "test_pairs = test_df[[\"Partner_id\", \"Product_id\", \"Scheme_Type\"]].drop_duplicates()\n",
    "recommendations = []\n",
    "\n",
    "for _, row in test_pairs.iterrows():\n",
    "    partner, product, current_scheme = row[\"Partner_id\"], row[\"Product_id\"], row[\"Scheme_Type\"]\n",
    "    if current_scheme in similarity_matrix.index:\n",
    "        similar_schemes = similarity_matrix.loc[current_scheme].drop(current_scheme).sort_values(ascending=False).head(3)\n",
    "        sim_list = similar_schemes.index.tolist()\n",
    "        recommendations.append({\n",
    "            \"Partner_id\": partner,\n",
    "            \"Product_id\": product,\n",
    "            \"Similarity_Score\": round(similar_schemes.mean(), 6),\n",
    "            \"Scheme_1\": sim_list[0] if len(sim_list) > 0 else \"No Scheme\",\n",
    "            \"Scheme_2\": sim_list[1] if len(sim_list) > 1 else \"No Scheme\",\n",
    "            \"Scheme_3\": sim_list[2] if len(sim_list) > 2 else \"No Scheme\"\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Scheme '{current_scheme}' not found in training data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ec5bb20-69be-438f-8ee9-81e16a5d8e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partner_id                   Product_id  Similarity_Score         Scheme_1  \\\n",
      "0      P1067             Modular Switches          0.593370  Volume Discount   \n",
      "1      P1003                          AIS          0.582308         Cashback   \n",
      "2      P1003                          ACB          0.039755  Volume Discount   \n",
      "3      P1003                          VCU          0.592733  Loyalty Program   \n",
      "4      P1063  Pump Starter and Controller          0.592733  Loyalty Program   \n",
      "\n",
      "         Scheme_2         Scheme_3  \n",
      "0  Seasonal Offer  Loyalty Program  \n",
      "1  Seasonal Offer  Loyalty Program  \n",
      "2  Seasonal Offer         Cashback  \n",
      "3        Cashback  Volume Discount  \n",
      "4        Cashback  Volume Discount  \n"
     ]
    }
   ],
   "source": [
    "# Save output\n",
    "recommendation_df = pd.DataFrame(recommendations)\n",
    "recommendation_df.to_csv(\"Scheme_Recommendations.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(recommendation_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60514d6-4a13-4269-a8a7-df844e0649ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c02fa8-afe2-4c81-80b5-58b16aee9337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce54f0ab-4fe6-4c16-b9ff-f5af9cf9c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Evaluation code\"\n",
    "# Import required library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb9b9d44-dcef-4cf6-9d2d-8b6fe374c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data (long format â€” one row per availed scheme per partner)\n",
    "test_df = pd.read_csv(\"Test_Data.csv\")\n",
    "\n",
    "# Load the recommendation data (top 3 recommended schemes per partner)\n",
    "rec_df = pd.read_csv(\"Scheme_Recommendations.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36905e5f-8dc7-4e74-8077-05560810d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Partner_id to get list of all availed schemes\n",
    "availed_df = (\n",
    "    test_df.groupby(\"Partner_id\")[\"Scheme_Type\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Scheme_Type\": \"Availed_Schemes\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "562a3210-1d3f-41c5-8d08-f4b0e5ee1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Scheme_1, Scheme_2, Scheme_3 into a single list column\n",
    "rec_df[\"Recommended_Schemes\"] = rec_df[[\"Scheme_1\", \"Scheme_2\", \"Scheme_3\"]].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "feed3bef-81f9-4737-9ea4-bd0ba49d5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge availed and recommended schemes using Partner_id\n",
    "df_all = pd.merge(\n",
    "    availed_df,\n",
    "    rec_df[[\"Partner_id\", \"Recommended_Schemes\"]],\n",
    "    on=\"Partner_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Ensure both lists are properly formatted\n",
    "df_all[\"Availed_Schemes\"] = df_all[\"Availed_Schemes\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df_all[\"Recommended_Schemes\"] = df_all[\"Recommended_Schemes\"].apply(lambda x: x if isinstance(x, list) else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a10df7e1-6453-4042-904f-ed68bcdb4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "k_list = [1, 2, 3]\n",
    "results = []\n",
    "\n",
    "# Evaluate precision, recall, F1 for each Top-K level\n",
    "for k in k_list:\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    for _, row in df_all.iterrows():\n",
    "        actual_set = set(row[\"Availed_Schemes\"])\n",
    "        recommended_k = row[\"Recommended_Schemes\"][:k]  # Top-K recommendations\n",
    "\n",
    "        if not actual_set:\n",
    "            continue  # skip if no availed schemes\n",
    "\n",
    "        # Count correct predictions in Top-K\n",
    "        tp = sum([1 for scheme in recommended_k if scheme in actual_set])\n",
    "        precision = tp / k\n",
    "        recall = tp / len(actual_set)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "   # Average the metrics across all partners\n",
    "    avg_precision = round(sum(precision_list) / len(precision_list), 4) if precision_list else 0\n",
    "    avg_recall = round(sum(recall_list) / len(recall_list), 4) if recall_list else 0\n",
    "    f1 = round(2 * avg_precision * avg_recall / (avg_precision + avg_recall), 4) if (avg_precision + avg_recall) else 0\n",
    "\n",
    "    results.append({\n",
    "        \"Top-K\": k,\n",
    "        \"Avg Precision\": avg_precision,\n",
    "        \"Avg Recall\": avg_recall,\n",
    "        \"Avg F1 Score\": f1\n",
    "    })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a067ba6-d80f-48eb-975b-6e3834559c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Per-Scheme Evaluation (WITH Availed Schemes) ====\n",
      "\n",
      "Top-1\n",
      "  Avg Precision : 0.9913\n",
      "  Avg Recall    : 0.245\n",
      "  Avg F1 Score  : 0.3929\n",
      "\n",
      "Top-2\n",
      "  Avg Precision : 0.9855\n",
      "  Avg Recall    : 0.4862\n",
      "  Avg F1 Score  : 0.6512\n",
      "\n",
      "Top-3\n",
      "  Avg Precision : 0.9875\n",
      "  Avg Recall    : 0.7312\n",
      "  Avg F1 Score  : 0.8402\n"
     ]
    }
   ],
   "source": [
    "# Print Top-K per-scheme evaluation metrics\n",
    "print(\"==== Per-Scheme Evaluation (WITH Availed Schemes) ====\")\n",
    "for r in results:\n",
    "    print(f\"\\nTop-{r['Top-K']}\")\n",
    "    print(f\"  Avg Precision : {r['Avg Precision']}\")\n",
    "    print(f\"  Avg Recall    : {r['Avg Recall']}\")\n",
    "    print(f\"  Avg F1 Score  : {r['Avg F1 Score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2bbbe-ea81-4216-b188-c12fe297c691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
